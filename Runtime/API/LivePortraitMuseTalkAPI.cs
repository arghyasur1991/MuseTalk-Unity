using System;
using System.Collections.Generic;
using System.Threading.Tasks;
using UnityEngine;

namespace MuseTalk.API
{
    using Core;
    using Models;
    using Utils;

    /// <summary>
    /// Input for the integrated LivePortrait + MuseTalk workflow
    /// </summary>
    public class LivePortraitMuseTalkInput
    {
        /// <summary>
        /// Single source image for LivePortrait animation generation
        /// </summary>
        public Texture2D SourceImage { get; set; }
        
        /// <summary>
        /// Driving frames that will control the animation (expressions, head movement)
        /// These will be used by LivePortrait to generate animated avatar textures
        /// </summary>
        public Texture2D[] DrivingFrames { get; set; }
        
        /// <summary>
        /// Audio clip for lip sync with MuseTalk
        /// </summary>
        public AudioClip AudioClip { get; set; }
        
        /// <summary>
        /// Batch size for MuseTalk processing
        /// </summary>
        public int BatchSize { get; set; } = 4;
        
        /// <summary>
        /// Whether to use composite output during LivePortrait generation
        /// </summary>
        public bool UseComposite { get; set; } = false;
        
        public LivePortraitMuseTalkInput(Texture2D sourceImage, Texture2D[] drivingFrames, AudioClip audioClip)
        {
            SourceImage = sourceImage ?? throw new ArgumentNullException(nameof(sourceImage));
            DrivingFrames = drivingFrames ?? throw new ArgumentNullException(nameof(drivingFrames));
            AudioClip = audioClip ?? throw new ArgumentNullException(nameof(audioClip));
        }
    }

    /// <summary>
    /// Result from the integrated LivePortrait + MuseTalk workflow
    /// </summary>
    public class LivePortraitMuseTalkResult
    {
        public bool Success { get; set; }
        public string ErrorMessage { get; set; }
        
        /// <summary>
        /// Intermediate result: Animated textures generated by LivePortrait
        /// </summary>
        public List<Texture2D> AnimatedTextures { get; set; } = new List<Texture2D>();
        
        /// <summary>
        /// Final result: Lip-synced talking head frames
        /// </summary>
        public List<Texture2D> TalkingHeadFrames { get; set; } = new List<Texture2D>();
        
        /// <summary>
        /// Performance metrics
        /// </summary>
        public WorkflowMetrics Metrics { get; set; } = new WorkflowMetrics();
    }

    /// <summary>
    /// Performance metrics for the integrated workflow
    /// </summary>
    public class WorkflowMetrics
    {
        public float LivePortraitDurationSeconds { get; set; }
        public float MuseTalkDurationSeconds { get; set; }
        public float TotalDurationSeconds => LivePortraitDurationSeconds + MuseTalkDurationSeconds;
        public int GeneratedAnimatedFrames { get; set; }
        public int GeneratedTalkingFrames { get; set; }
    }

    /// <summary>
    /// Integrated API that combines LivePortrait and MuseTalk for complete talking head generation
    /// 
    /// Workflow:
    /// 1. LivePortrait: Generate animated textures from single source image + driving frames
    /// 2. MuseTalk: Apply lip sync to the animated textures using audio
    /// 
    /// This matches the user's requested workflow exactly
    /// </summary>
    public class LivePortraitMuseTalkAPI : IDisposable
    {
        private static readonly DebugLogger Logger = new();
        
        private LivePortraitInference _livePortrait;
        private MuseTalkInference _museTalk;
        private MuseTalkConfig _config;
        private bool _initialized = false;
        private bool _disposed = false;
        
        public bool IsInitialized => _initialized;
        
        /// <summary>
        /// Initialize the integrated API with configuration
        /// </summary>
        public LivePortraitMuseTalkAPI(MuseTalkConfig config)
        {
            _config = config ?? throw new ArgumentNullException(nameof(config));
            
            try
            {
                Logger.Log("[LivePortraitMuseTalkAPI] Initializing integrated workflow...");
                
                // Initialize LivePortrait inference
                _livePortrait = new LivePortraitInference(_config);
                
                // Initialize MuseTalk inference
                _museTalk = new MuseTalkInference(_config);
                
                // Verify both systems are initialized
                if (!_livePortrait.IsInitialized)
                {
                    throw new InvalidOperationException("LivePortrait inference failed to initialize");
                }
                
                if (!_museTalk.IsInitialized)
                {
                    throw new InvalidOperationException("MuseTalk inference failed to initialize");
                }
                
                _initialized = true;
                Logger.Log("[LivePortraitMuseTalkAPI] Successfully initialized integrated workflow");
            }
            catch (Exception e)
            {
                Logger.LogError($"[LivePortraitMuseTalkAPI] Failed to initialize: {e.Message}");
                _initialized = false;
            }
        }
        
        /// <summary>
        /// Generate talking head animation using the integrated LivePortrait + MuseTalk workflow
        /// 
        /// Step 1: LivePortrait generates animated textures from source image + driving frames
        /// Step 2: MuseTalk applies lip sync to animated textures using audio
        /// </summary>
        public async Task<LivePortraitMuseTalkResult> GenerateAsync(LivePortraitMuseTalkInput input)
        {
            if (!_initialized)
                throw new InvalidOperationException("API not initialized");
                
            if (input?.SourceImage == null || input.DrivingFrames == null || input.AudioClip == null)
                throw new ArgumentException("Invalid input: source image, driving frames, and audio are required");
                
            var result = new LivePortraitMuseTalkResult();
            var startTime = Time.realtimeSinceStartup;
            
            try
            {
                Logger.Log($"[LivePortraitMuseTalkAPI] === STARTING INTEGRATED WORKFLOW ===");
                Logger.Log($"[LivePortraitMuseTalkAPI] Source: {input.SourceImage.width}x{input.SourceImage.height}");
                Logger.Log($"[LivePortraitMuseTalkAPI] Driving frames: {input.DrivingFrames.Length}");
                Logger.Log($"[LivePortraitMuseTalkAPI] Audio: {input.AudioClip.name} ({input.AudioClip.length:F2}s)");
                
                // STAGE 1: Generate animated textures using LivePortrait
                Logger.Log("[LivePortraitMuseTalkAPI] STAGE 1: Generating animated textures with LivePortrait...");
                var livePortraitStartTime = Time.realtimeSinceStartup;
                
                var livePortraitInput = new LivePortraitInput
                {
                    SourceImage = input.SourceImage,
                    DrivingFrames = input.DrivingFrames,
                    UseComposite = input.UseComposite
                };
                
                var livePortraitResult = await _livePortrait.GenerateAsync(livePortraitInput);
                var livePortraitEndTime = Time.realtimeSinceStartup;
                
                if (!livePortraitResult.Success)
                {
                    result.Success = false;
                    result.ErrorMessage = $"LivePortrait generation failed: {livePortraitResult.ErrorMessage}";
                    return result;
                }
                
                result.AnimatedTextures = livePortraitResult.GeneratedFrames;
                result.Metrics.LivePortraitDurationSeconds = livePortraitEndTime - livePortraitStartTime;
                result.Metrics.GeneratedAnimatedFrames = livePortraitResult.GeneratedFrames.Count;
                
                Logger.Log($"[LivePortraitMuseTalkAPI] Stage 1 completed - Generated {result.AnimatedTextures.Count} animated textures in {result.Metrics.LivePortraitDurationSeconds:F2}s");
                
                // STAGE 2: Apply lip sync using MuseTalk
                Logger.Log("[LivePortraitMuseTalkAPI] STAGE 2: Applying lip sync with MuseTalk...");
                var museTalkStartTime = Time.realtimeSinceStartup;
                
                var museTalkInput = new MuseTalkInput(result.AnimatedTextures.ToArray(), input.AudioClip)
                {
                    BatchSize = input.BatchSize
                };
                
                var museTalkResult = await _museTalk.GenerateAsync(museTalkInput);
                var museTalkEndTime = Time.realtimeSinceStartup;
                
                if (!museTalkResult.Success)
                {
                    result.Success = false;
                    result.ErrorMessage = $"MuseTalk generation failed: {museTalkResult.ErrorMessage}";
                    return result;
                }
                
                result.TalkingHeadFrames = museTalkResult.GeneratedFrames;
                result.Metrics.MuseTalkDurationSeconds = museTalkEndTime - museTalkStartTime;
                result.Metrics.GeneratedTalkingFrames = museTalkResult.GeneratedFrames.Count;
                
                Logger.Log($"[LivePortraitMuseTalkAPI] Stage 2 completed - Generated {result.TalkingHeadFrames.Count} talking head frames in {result.Metrics.MuseTalkDurationSeconds:F2}s");
                
                // Success!
                result.Success = true;
                var totalEndTime = Time.realtimeSinceStartup;
                var totalDuration = totalEndTime - startTime;
                
                Logger.Log($"[LivePortraitMuseTalkAPI] === WORKFLOW COMPLETED ===");
                Logger.Log($"[LivePortraitMuseTalkAPI] Total time: {totalDuration:F2}s");
                Logger.Log($"[LivePortraitMuseTalkAPI] LivePortrait: {result.Metrics.LivePortraitDurationSeconds:F2}s ({result.Metrics.LivePortraitDurationSeconds/totalDuration*100:F1}%)");
                Logger.Log($"[LivePortraitMuseTalkAPI] MuseTalk: {result.Metrics.MuseTalkDurationSeconds:F2}s ({result.Metrics.MuseTalkDurationSeconds/totalDuration*100:F1}%)");
                Logger.Log($"[LivePortraitMuseTalkAPI] Animated frames: {result.Metrics.GeneratedAnimatedFrames}");
                Logger.Log($"[LivePortraitMuseTalkAPI] Talking frames: {result.Metrics.GeneratedTalkingFrames}");
                
                return result;
            }
            catch (Exception e)
            {
                Logger.LogError($"[LivePortraitMuseTalkAPI] Workflow failed: {e.Message}");
                result.Success = false;
                result.ErrorMessage = e.Message;
                return result;
            }
        }
        
        /// <summary>
        /// Generate only animated textures using LivePortrait (Stage 1 only)
        /// Useful for previewing or when lip sync is not needed
        /// </summary>
        public async Task<LivePortraitResult> GenerateAnimatedTexturesAsync(Texture2D sourceImage, Texture2D[] drivingFrames, bool useComposite = false)
        {
            if (!_initialized)
                throw new InvalidOperationException("API not initialized");
                
            var input = new LivePortraitInput
            {
                SourceImage = sourceImage,
                DrivingFrames = drivingFrames,
                UseComposite = useComposite
            };
            
            return await _livePortrait.GenerateAsync(input);
        }
        
        /// <summary>
        /// Apply lip sync to existing animated textures using MuseTalk (Stage 2 only)
        /// Useful when you already have animated textures and want to add lip sync
        /// </summary>
        public async Task<MuseTalkResult> ApplyLipSyncAsync(Texture2D[] animatedTextures, AudioClip audioClip, int batchSize = 4)
        {
            if (!_initialized)
                throw new InvalidOperationException("API not initialized");
                
            var input = new MuseTalkInput(animatedTextures, audioClip)
            {
                BatchSize = batchSize
            };
            
            return await _museTalk.GenerateAsync(input);
        }
        
        /// <summary>
        /// Create a video sequence from generated frames
        /// Note: This would require additional video encoding functionality
        /// Returns the frame count for now since Unity doesn't have built-in VideoClip creation
        /// </summary>
        public int CreateVideoSequence(List<Texture2D> frames, float frameRate = 25f, AudioClip audioClip = null)
        {
            // This would require implementing video encoding in Unity
            // For now, just log the request and return frame count
            Logger.Log($"[LivePortraitMuseTalkAPI] Video creation requested: {frames.Count} frames at {frameRate} FPS");
            Logger.LogWarning("[LivePortraitMuseTalkAPI] Video creation not yet implemented - frames are available as Texture2D list");
            return frames.Count;
        }
        
        /// <summary>
        /// Get cache information for debugging and monitoring
        /// </summary>
        public string GetCacheInfo()
        {
            if (!_initialized) return "API not initialized";
            
            var livePortraitInfo = "LivePortrait: No cache info";
            var museTalkInfo = _museTalk?.GetCacheInfo() ?? "MuseTalk: No cache info";
            
            return $"{livePortraitInfo} | {museTalkInfo}";
        }
        
        /// <summary>
        /// Clear all caches to free memory
        /// </summary>
        public async Task ClearCachesAsync()
        {
            if (_museTalk != null)
            {
                await _museTalk.ClearDiskCacheAsync();
                MuseTalkInference.ClearAvatarAnimationCache();
            }
            
            Logger.Log("[LivePortraitMuseTalkAPI] Cleared all caches");
        }
        
        /// <summary>
        /// Get performance statistics for the last workflow run
        /// </summary>
        public string GetPerformanceInfo(WorkflowMetrics metrics)
        {
            if (metrics == null) return "No metrics available";
            
            return $"Total: {metrics.TotalDurationSeconds:F2}s | " +
                   $"LivePortrait: {metrics.LivePortraitDurationSeconds:F2}s ({metrics.GeneratedAnimatedFrames} frames) | " +
                   $"MuseTalk: {metrics.MuseTalkDurationSeconds:F2}s ({metrics.GeneratedTalkingFrames} frames) | " +
                   $"Efficiency: {(metrics.GeneratedTalkingFrames / metrics.TotalDurationSeconds):F1} frames/sec";
        }
        
        public void Dispose()
        {
            if (!_disposed)
            {
                _livePortrait?.Dispose();
                _museTalk?.Dispose();
                _disposed = true;
                Logger.Log("[LivePortraitMuseTalkAPI] Disposed");
            }
        }
        
        ~LivePortraitMuseTalkAPI()
        {
            Dispose();
        }
    }
    
    /// <summary>
    /// Factory for creating LivePortraitMuseTalkAPI instances
    /// </summary>
    public static class LivePortraitMuseTalkFactory
    {
        /// <summary>
        /// Create an instance of the integrated API with default configuration
        /// </summary>
        public static LivePortraitMuseTalkAPI Create(string modelPath = "MuseTalk")
        {
            var config = new MuseTalkConfig(modelPath);
            return new LivePortraitMuseTalkAPI(config);
        }
        
        /// <summary>
        /// Create an instance optimized for performance
        /// </summary>
        public static LivePortraitMuseTalkAPI CreateOptimized(string modelPath = "MuseTalk")
        {
            var config = MuseTalkConfig.CreateOptimized(modelPath);
            return new LivePortraitMuseTalkAPI(config);
        }
        
        /// <summary>
        /// Create an instance optimized for development/debugging
        /// </summary>
        public static LivePortraitMuseTalkAPI CreateForDevelopment(string modelPath = "MuseTalk")
        {
            var config = MuseTalkConfig.CreateForDevelopment(modelPath);
            return new LivePortraitMuseTalkAPI(config);
        }
    }
}
