using System;
using System.Collections.Concurrent;
using System.Collections.Generic;
using System.Threading;
using System.Threading.Tasks;
using UnityEngine;

namespace MuseTalk.API
{
    using Core;
    using Models;
    using Utils;

    /// <summary>
    /// Input for the integrated LivePortrait + MuseTalk workflow
    /// </summary>
    public class LivePortraitMuseTalkInput
    {
        /// <summary>
        /// Single source image for LivePortrait animation generation
        /// </summary>
        public Texture2D SourceImage { get; set; }
        
        /// <summary>
        /// Driving frames that will control the animation (expressions, head movement)
        /// These will be used by LivePortrait to generate animated avatar textures
        /// </summary>
        public Texture2D[] DrivingFrames { get; set; }
        
        /// <summary>
        /// Audio clip for lip sync with MuseTalk
        /// </summary>
        public AudioClip AudioClip { get; set; }
        
        /// <summary>
        /// Batch size for MuseTalk processing
        /// </summary>
        public int BatchSize { get; set; } = 4;
        
        public LivePortraitMuseTalkInput(Texture2D sourceImage, Texture2D[] drivingFrames, AudioClip audioClip)
        {
            SourceImage = sourceImage ?? throw new ArgumentNullException(nameof(sourceImage));
            DrivingFrames = drivingFrames ?? throw new ArgumentNullException(nameof(drivingFrames));
            AudioClip = audioClip ?? throw new ArgumentNullException(nameof(audioClip));
        }
    }

    /// <summary>
    /// Result from the integrated LivePortrait + MuseTalk workflow
    /// </summary>
    public class LivePortraitMuseTalkResult
    {
        public bool Success { get; set; }
        public string ErrorMessage { get; set; }
        
        /// <summary>
        /// Intermediate result: Animated textures generated by LivePortrait
        /// </summary>
        public List<Texture2D> AnimatedTextures { get; set; } = new List<Texture2D>();
        
        /// <summary>
        /// Final result: Lip-synced talking head frames
        /// </summary>
        public List<Texture2D> TalkingHeadFrames { get; set; } = new List<Texture2D>();
        
        /// <summary>
        /// Performance metrics
        /// </summary>
        public WorkflowMetrics Metrics { get; set; } = new WorkflowMetrics();
    }

    /// <summary>
    /// Performance metrics for the integrated workflow
    /// </summary>
    public class WorkflowMetrics
    {
        public float LivePortraitDurationSeconds { get; set; }
        public float MuseTalkDurationSeconds { get; set; }
        public float TotalDurationSeconds => LivePortraitDurationSeconds + MuseTalkDurationSeconds;
        public int GeneratedAnimatedFrames { get; set; }
        public int GeneratedTalkingFrames { get; set; }
    }

    public sealed class LivePortaitStream
    {
        public int TotalExpectedFrames { get; set; }

        public LivePortaitStream(int totalExpectedFrames)
        {
            TotalExpectedFrames = totalExpectedFrames;
        }

        internal readonly ConcurrentQueue<Texture2D> queue = new();
        internal CancellationTokenSource cts = new();

        public bool Finished { get; internal set; }

        /// Non-blocking poll. Returns false if no frame is ready yet.
        public bool TryGetNext(out Texture2D tex) => queue.TryDequeue(out tex);

        /// Yield instruction that waits until the *next* frame exists,
        /// then exposes it through the .Texture property.
        public FrameAwaiter WaitForNext() => new(queue);
    }

    /// Custom yield instruction that delivers one Texture2D.
    public sealed class FrameAwaiter : CustomYieldInstruction
    {
        private readonly ConcurrentQueue<Texture2D> _q;
        public Texture2D Texture { get; private set; }

        public FrameAwaiter(ConcurrentQueue<Texture2D> q) => _q = q;

        public override bool keepWaiting
        {
            get
            {
                if (_q.TryDequeue(out var tex))
                {
                    Texture = tex;
                    return false;          // stop waiting â€“ caller resumes
                }
                return true;               // keep waiting this frame
            }
        }
    }

    /// <summary>
    /// Stream for driving frames input - similar to output stream but for input processing
    /// </summary>
    public sealed class DrivingFramesStream
    {
        public int TotalExpectedFrames { get; set; }
        public bool LoadingFinished { get; internal set; }
        public bool ProcessingFinished { get; internal set; }

        public DrivingFramesStream(int totalExpectedFrames)
        {
            TotalExpectedFrames = totalExpectedFrames;
        }

        internal readonly ConcurrentQueue<Texture2D> loadQueue = new();
        internal CancellationTokenSource cts = new();

        /// Non-blocking poll. Returns false if no frame is ready yet.
        public bool TryGetNext(out Texture2D tex) => loadQueue.TryDequeue(out tex);

        /// Check if frames are available for processing
        public bool HasFramesAvailable => !loadQueue.IsEmpty;

        /// Get current queue count
        public int QueueCount => loadQueue.Count;

        /// Yield instruction that waits until the *next* frame exists,
        /// then exposes it through the .Texture property.
        public FrameAwaiter WaitForNext() => new(loadQueue);

        /// Check if we have more frames to process
        public bool HasMoreFrames => !LoadingFinished || HasFramesAvailable;
    }

    /// <summary>
    /// Integrated API that combines LivePortrait and MuseTalk for complete talking head generation
    /// 
    /// Workflow:
    /// 1. LivePortrait: Generate animated textures from single source image + driving frames
    /// 2. MuseTalk: Apply lip sync to the animated textures using audio
    /// 
    /// This matches the user's requested workflow exactly
    /// </summary>
    public class LivePortraitMuseTalkAPI : IDisposable
    {
        private static readonly DebugLogger Logger = new();
        
        private LivePortraitInference _livePortrait;
        private MuseTalkInference _museTalk;
        private MuseTalkConfig _config;
        private bool _initialized = false;
        private bool _disposed = false;
        private readonly AvatarController _avatarController;
        
        public bool IsInitialized => _initialized;
        
        /// <summary>
        /// Initialize the integrated API with configuration
        /// </summary>
        public LivePortraitMuseTalkAPI(MuseTalkConfig config, AvatarController avatarController)
        {
            _config = config ?? throw new ArgumentNullException(nameof(config));
            _avatarController = avatarController ?? throw new ArgumentNullException(nameof(avatarController));
            
            try
            {
                Logger.Log("[LivePortraitMuseTalkAPI] Initializing integrated workflow...");
                
                // Initialize LivePortrait inference
                _livePortrait = new LivePortraitInference(_config);
                
                // Initialize MuseTalk inference
                _museTalk = new MuseTalkInference(_config);
                
                // Verify both systems are initialized
                if (!_livePortrait.IsInitialized)
                {
                    throw new InvalidOperationException("LivePortrait inference failed to initialize");
                }
                
                if (!_museTalk.IsInitialized)
                {
                    throw new InvalidOperationException("MuseTalk inference failed to initialize");
                }
                
                _initialized = true;
                Logger.Log("[LivePortraitMuseTalkAPI] Successfully initialized integrated workflow");
            }
            catch (Exception e)
            {
                Logger.LogError($"[LivePortraitMuseTalkAPI] Failed to initialize: {e.Message}");
                _initialized = false;
            }
        }
        
        /// <summary>
        /// Generate animated textures only using LivePortrait (SYNCHRONOUS) - List<Texture2D> overload
        /// </summary>
        public LivePortaitStream GenerateAnimatedTexturesAsync(Texture2D sourceImage, List<Texture2D> drivingFrames)
        {
            if (!_initialized)
                throw new InvalidOperationException("API not initialized");
                
            if (sourceImage == null || drivingFrames == null)
                throw new ArgumentException("Invalid input: source image and driving frames are required");
                
            Logger.Log($"[LivePortraitMuseTalkAPI] Generating animated textures (SYNC): {drivingFrames.Count} driving frames");
            
            var input = new LivePortraitInput
            {
                SourceImage = sourceImage,
                DrivingFrames = drivingFrames
            };

            var stream = new LivePortaitStream(drivingFrames.Count);
            _avatarController.StartCoroutine(_livePortrait.GenerateAsync(input, stream));
            return stream;
        }

        public LivePortaitStream GenerateAnimatedTexturesAsync(Texture2D sourceImage, string drivingFramesPath, int maxFrames = -1)
        {
            if (!_initialized)
                throw new InvalidOperationException("API not initialized");
                
            if (sourceImage == null || string.IsNullOrEmpty(drivingFramesPath))
                throw new ArgumentException("Invalid input: source image and driving frames path are required");

            // Get frame count first to estimate total frames
            var frameFiles = FileUtils.GetFrameFiles(drivingFramesPath, maxFrames); // Send maxFrames > 0 to load some frames
            if (frameFiles.Length == 0)
            {
                throw new ArgumentException($"No driving frames found in path: {drivingFramesPath}");
            }

            Logger.Log($"[LivePortraitMuseTalkAPI] Starting pipelined processing: {frameFiles.Length} driving frames");
            
            var stream = new LivePortaitStream(frameFiles.Length);
            _avatarController.StartCoroutine(GenerateAnimatedTexturesPipelined(sourceImage, frameFiles, stream));
            return stream;
        }

        /// <summary>
        /// Pipelined generation that starts source processing immediately and streams driving frames
        /// </summary>
        private System.Collections.IEnumerator GenerateAnimatedTexturesPipelined(Texture2D sourceImage, string[] frameFiles, LivePortaitStream outputStream)
        {
            // Step 1: Start source image processing immediately (async)
            var (srcImg, srcWidth, srcHeight) = TextureUtils.Texture2DToBytes(sourceImage);
            var processSrcTask = _livePortrait.ProcessSourceImageAsync(srcImg, srcWidth, srcHeight);
            
            Logger.Log("[LivePortraitMuseTalkAPI] Source image processing started asynchronously");

            // Step 2: Create driving frames stream and start loading frames asynchronously
            var drivingStream = new DrivingFramesStream(frameFiles.Length);
            var loadFramesCoroutine = _avatarController.StartCoroutine(LoadDrivingFramesAsync(frameFiles, drivingStream));

            // Step 3: Wait for source processing to complete
            yield return new WaitUntil(() => processSrcTask.IsCompleted);
            var processResult = processSrcTask.Result;
            
            Logger.Log("[LivePortraitMuseTalkAPI] Source image processing completed, starting frame processing pipeline");

            // Step 4: Process driving frames as they become available
            int processedFrames = 0;
            var predInfo = new LivePortraitPredInfo
            {
                Landmarks = null,
                InitialMotionInfo = null
            };

            while (processedFrames < frameFiles.Length && drivingStream.HasMoreFrames)
            {
                // Wait for next driving frame using CustomYieldInstruction
                var awaiter = drivingStream.WaitForNext();
                yield return awaiter;

                if (awaiter.Texture != null)
                {
                    var drivingFrame = awaiter.Texture;
                    
                    // Process this driving frame
                    var (imgRgbData, w, h) = TextureUtils.Texture2DToBytes(drivingFrame);
                    
                    var predictTask = _livePortrait.ProcessNextFrameAsync(processResult, predInfo, imgRgbData, w, h);
                    yield return new WaitUntil(() => predictTask.IsCompleted);
                    var (generatedImg, updatedPredInfo) = predictTask.Result;
                    predInfo = updatedPredInfo;

                    // Output the generated frame
                    if (generatedImg != null)
                    {
                        var generatedImgTexture = TextureUtils.BytesToTexture2D(generatedImg, processResult.SrcImgWidth, processResult.SrcImgHeight);
                        outputStream.queue.Enqueue(generatedImgTexture);
                        Logger.Log($"[LivePortraitMuseTalkAPI] Processed frame {processedFrames + 1}/{frameFiles.Length}");
                    }

                    processedFrames++;
                    
                    // Clean up driving frame
                    if (drivingFrame != null)
                    {
                        UnityEngine.Object.DestroyImmediate(drivingFrame);
                    }
                }
                else if (drivingStream.LoadingFinished)
                {
                    // No more frames available and loading is finished
                    break;
                }
            }

            // Mark streams as finished
            outputStream.Finished = true;
            drivingStream.ProcessingFinished = true;
            
            Logger.Log($"[LivePortraitMuseTalkAPI] Pipelined processing completed: {processedFrames} frames generated");
        }

        /// <summary>
        /// Load driving frames asynchronously and add them to the stream
        /// </summary>
        private System.Collections.IEnumerator LoadDrivingFramesAsync(string[] frameFiles, DrivingFramesStream stream)
        {
            Logger.Log($"[LivePortraitMuseTalkAPI] Starting to load {frameFiles.Length} driving frames asynchronously");

            for (int i = 0; i < frameFiles.Length; i++)
            {
                string filePath = frameFiles[i];
                
                // Load frame data asynchronously - outside try-catch to avoid yield in try-catch
                var loadFileTask = System.IO.File.ReadAllBytesAsync(filePath);
                yield return new WaitUntil(() => loadFileTask.IsCompleted);
                
                try
                {
                    if (loadFileTask.IsFaulted)
                    {
                        Logger.LogError($"[LivePortraitMuseTalkAPI] Error loading driving frame {filePath}: {loadFileTask.Exception?.GetBaseException().Message}");
                        continue;
                    }
                    
                    byte[] fileData = loadFileTask.Result;
                    Texture2D texture = new(2, 2);
                    
                    if (texture.LoadImage(fileData))
                    {
                        texture.name = System.IO.Path.GetFileNameWithoutExtension(filePath);
                        var rgbTexture = TextureUtils.ConvertTexture2DToRGB24(texture);
                        stream.loadQueue.Enqueue(rgbTexture);
                        
                        // Clean up original texture if different
                        if (rgbTexture != texture)
                        {
                            UnityEngine.Object.DestroyImmediate(texture);
                        }
                    }
                    else
                    {
                        Logger.LogWarning($"[LivePortraitMuseTalkAPI] Failed to load image: {filePath}");
                        UnityEngine.Object.DestroyImmediate(texture);
                    }
                }
                catch (Exception e)
                {
                    Logger.LogError($"[LivePortraitMuseTalkAPI] Error processing driving frame {filePath}: {e.Message}");
                }
                yield return null;
            }

            stream.LoadingFinished = true;
            Logger.Log($"[LivePortraitMuseTalkAPI] Finished loading driving frames, {stream.QueueCount} frames queued");
        }

        /// <summary>
        /// Get cache information for debugging and monitoring
        /// </summary>
        public string GetCacheInfo()
        {
            if (!_initialized) return "API not initialized";
            
            var livePortraitInfo = "LivePortrait: No cache info";
            var museTalkInfo = _museTalk?.GetCacheInfo() ?? "MuseTalk: No cache info";
            
            return $"{livePortraitInfo} | {museTalkInfo}";
        }
        
        /// <summary>
        /// Clear all caches to free memory
        /// </summary>
        public async Task ClearCachesAsync()
        {
            if (_museTalk != null)
            {
                await _museTalk.ClearDiskCacheAsync();
                MuseTalkInference.ClearAvatarAnimationCache();
            }
            
            Logger.Log("[LivePortraitMuseTalkAPI] Cleared all caches");
        }
        
        /// <summary>
        /// Get performance statistics for the last workflow run
        /// </summary>
        public string GetPerformanceInfo(WorkflowMetrics metrics)
        {
            if (metrics == null) return "No metrics available";
            
            return $"Total: {metrics.TotalDurationSeconds:F2}s | " +
                   $"LivePortrait: {metrics.LivePortraitDurationSeconds:F2}s ({metrics.GeneratedAnimatedFrames} frames) | " +
                   $"MuseTalk: {metrics.MuseTalkDurationSeconds:F2}s ({metrics.GeneratedTalkingFrames} frames) | " +
                   $"Efficiency: {(metrics.GeneratedTalkingFrames / metrics.TotalDurationSeconds):F1} frames/sec";
        }
        
        public void Dispose()
        {
            if (!_disposed)
            {
                _livePortrait?.Dispose();
                _museTalk?.Dispose();
                _disposed = true;
                Logger.Log("[LivePortraitMuseTalkAPI] Disposed");
            }
        }
        
        ~LivePortraitMuseTalkAPI()
        {
            Dispose();
        }
    }
    
    /// <summary>
    /// Factory for creating LivePortraitMuseTalkAPI instances
    /// </summary>
    public static class LivePortraitMuseTalkFactory
    {
        /// <summary>
        /// Create an instance of the integrated API with default configuration
        /// </summary>
        public static LivePortraitMuseTalkAPI Create(AvatarController avatarController,string modelPath = "MuseTalk")
        {
            var config = new MuseTalkConfig(modelPath);
            return new LivePortraitMuseTalkAPI(config, avatarController);
        }
        
        /// <summary>
        /// Create an instance optimized for performance
        /// </summary>
        public static LivePortraitMuseTalkAPI CreateOptimized(AvatarController avatarController, string modelPath = "MuseTalk")
        {
            var config = MuseTalkConfig.CreateOptimized(modelPath);
            return new LivePortraitMuseTalkAPI(config, avatarController);
        }
        
        /// <summary>
        /// Create an instance optimized for development/debugging
        /// </summary>
        public static LivePortraitMuseTalkAPI CreateForDevelopment(AvatarController avatarController, string modelPath = "MuseTalk")
        {
            var config = MuseTalkConfig.CreateForDevelopment(modelPath);
            return new LivePortraitMuseTalkAPI(config, avatarController);
        }
    }
}
